{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c7236a86-3eb4-4788-ace4-1b8cdc37d6d0"
    }
   },
   "source": [
    "# Downloading and preprocessing training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data is Qur'an Verses. We have found a website that separated the Qur'an into verse by verse.It's URL is http://www.everyayah.com/data/status.php. It has a standard way for naming the files, first http://www.everyayah.com/data, then the name of the reader and may be some more informations like is it mujawwad or murattal e.g. /Abdul_Basit_Mujawwad_128kbps, then the number of the Surah and the number of the verse followed by .mp3 e.g. /001001.mp3. So we have made a web crawling python script to download the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded and format the data in a standard way like LibriSpeech and other speech recognition data sets. Fist separated files into folders depending on who is the reader, then separated each reader folder into other folders grouping related files. We choosed this grouping to be the verses of each Surah, And named each file by the order of the reader then the order of the Surah then its order in the Surah."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pathlib.Path(path).mkdir(parents=True, exist_ok=True) to check if the required folder is not found then create it.\n",
    "And we will use also urllib.request.urlretrieve(url, file_name) to download the file in this URL to the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install and import urllib.request and pathlib to request the data from the web site and to create required folders to download data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6bf7af20-39b8-45d2-816b-c76ada905199"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make a list with all readers in it and another one with the number of verses in each Surah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "da764200-fc01-44cb-bb1e-6940adcdd758"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Audios are with different rates, so I ordered them.\n",
    "index(0) 16kbps\n",
    "indices(1,6) 32kbps\n",
    "indices(7,8) 40kbps\n",
    "indices(9,10) 48kbps\n",
    "indices(11,32) 64kbps\n",
    "indices(33,60) 128kbps\n",
    "indices(61,67) 192kbps\n",
    "'''\n",
    "\n",
    "reader = [\"Menshawi_16kbps\",\n",
    "    \"Abdullah_Basfar_32kbps\",\n",
    "    \"Hudhaify_32kbps\",\n",
    "    \"Ibrahim_Akhdar_32kbps\",\n",
    "    \"Menshawi_32kbps\",\n",
    "    \"Muhammad_Ayyoub_32kbps\",\n",
    "    \"mahmoud_ali_al_banna_32kbps\",\n",
    "    \"Ghamadi_40kbps\",\n",
    "    \"Karim_Mansoori_40kbps\",\n",
    "    \"Mustafa_Ismail_48kbps\",\n",
    "    \"Parhizgar_48kbps\",\n",
    "    \"AbdulSamad_64kbps_QuranExplorer.Com\",\n",
    "    \"Abdul_Basit_Murattal_64kbps\",\n",
    "    \"Abdullah_Basfar_64kbps\",\n",
    "    \"Abdurrahmaan_As-Sudais_64kbps\",\n",
    "    \"Abu_Bakr_Ash-Shaatree_64kbps\",\n",
    "    \"Ahmed_ibn_Ali_al-Ajamy_64kbps_QuranExplorer.Com\",\n",
    "    \"Alafasy_64kbps\",\n",
    "    \"Ali_Jaber_64kbps\",\n",
    "    \"Ayman_Sowaid_64kbps\",\n",
    "    \"Fares_Abbad_64kbps\",\n",
    "    \"Hani_Rifai_64kbps\",\n",
    "    \"Hudhaify_64kbps\",\n",
    "    \"Husary_64kbps\",\n",
    "    \"Husary_Mujawwad_64kbps\",\n",
    "    \"Ibrahim_Akhdar_64kbps\",\n",
    "    \"Maher_AlMuaiqly_64kbps\",\n",
    "    \"Minshawy_Mujawwad_64kbps\",\n",
    "    \"Mohammad_al_Tablaway_64kbps\",\n",
    "    \"Muhammad_Ayyoub_64kbps\",\n",
    "    \"Muhammad_Jibreel_64kbps\",\n",
    "    \"Saood_ash-Shuraym_64kbps\",\n",
    "    \"khalefa_al_tunaiji_64kbps\",\n",
    "    \"Abdul_Basit_Mujawwad_128kbps\",\n",
    "    \"Abdullaah_3awwaad_Al-Juhaynee_128kbps\",\n",
    "    \"Abdullah_Matroud_128kbps\",\n",
    "    \"Abu_Bakr_Ash-Shaatree_128kbps\",\n",
    "    \"Ahmed_Neana_128kbps\",\n",
    "    \"Ahmed_ibn_Ali_al-Ajamy_128kbps_ketaballah.net\",\n",
    "    \"Akram_AlAlaqimy_128kbps\",\n",
    "    \"Alafasy_128kbps\",\n",
    "    \"Ali_Hajjaj_AlSuesy_128kbps\",\n",
    "    \"Hudhaify_128kbps\",\n",
    "    \"Husary_128kbps\",\n",
    "    \"Husary_128kbps_Mujawwad\",\n",
    "    \"Husary_Muallim_128kbps\",\n",
    "    \"MaherAlMuaiqly128kbps\",\n",
    "    \"Minshawy_Murattal_128kbps\",\n",
    "    \"Mohammad_al_Tablaway_128kbps\",\n",
    "    \"Muhammad_AbdulKareem_128kbps\",\n",
    "    \"Muhammad_Ayyoub_128kbps\",\n",
    "    \"Muhammad_Jibreel_128kbps\",\n",
    "    \"Nasser_Alqatami_128kbps\",\n",
    "    \"Sahl_Yassin_128kbps\",\n",
    "    \"Salaah_AbdulRahman_Bukhatir_128kbps\",\n",
    "    \"Salah_Al_Budair_128kbps\",\n",
    "    \"Saood_ash-Shuraym_128kbps\",\n",
    "    \"Yaser_Salamah_128kbps\",\n",
    "    \"Yasser_Ad-Dussary_128kbps\",\n",
    "    \"ahmed_ibn_ali_al_ajamy_128kbps\",\n",
    "    \"aziz_alili_128kbps\",\n",
    "    \"Abdul_Basit_Murattal_192kbps\",\n",
    "    \"Abdullah_Basfar_192kbps\",\n",
    "    \"Abdurrahmaan_As-Sudais_192kbps\",\n",
    "    \"Hani_Rifai_192kbps\",\n",
    "    \"Khaalid_Abdullaah_al-Qahtaanee_192kbps\",\n",
    "    \"Minshawy_Mujawwad_192kbps\",\n",
    "    \"Muhsin_Al_Qasim_192kbps\"]\n",
    "\n",
    "num_of_verses = [7, 286, 200, 176, 120, 165, 206, 75, 129, 109, 123, 111,\n",
    "                 43, 52,    99, 128, 111, 110, 98, 135, 112, 78, 118, 64,\n",
    "                 77, 227, 93, 88, 69, 60, 34, 30, 73, 54, 45, 83, 182, 88,\n",
    "                 75, 85, 54, 53, 89, 59, 37, 35, 38, 29, 18, 45, 60, 49,\n",
    "                 62, 55, 78, 96, 29, 22, 24, 13, 14, 11, 11, 18, 12, 12,\n",
    "                 30, 52, 2, 44, 28, 28, 20, 56, 40, 31, 50, 40, 46, 42,\n",
    "                 29, 19, 36, 25, 22, 17, 19, 26, 30, 20, 15, 21, 11, 8,\n",
    "                 8, 19, 5, 8, 8, 11, 11, 8, 3, 9, 5, 4, 7, 3, 6, 3, 5, 4,\n",
    "                 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then iterate over selected readers, Surahs and verses of each selected Surah and download it in the proper folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5e84a380-d72c-414c-8060-6538a4a80626"
    }
   },
   "outputs": [],
   "source": [
    "#here I select the first 7 readers in the list\n",
    "for i in range(0,7):\n",
    "    #select the required surahs\n",
    "    #here I select Al-Fatiha, Al-Ikhlas, Al-Falaq, and An-Nas\n",
    "    for j in [0, 111,112, 113]:\n",
    "        #iterate over all verses in each surah\n",
    "        for k in range(num_of_verses[j]):\n",
    "            #example of downloadable links\n",
    "            #http://www.everyayah.com/data/Alafasy_64kbps/001001.mp3\n",
    "            path = 'data/'+str(i+1)+\"/\"+str(j+1)+\"/\"\n",
    "            pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "            url = \"http://www.everyayah.com/data/\" + reader[i] + \\\n",
    "                \"/\" + str(\"{0:0=3d}\".format(j+1)) \\\n",
    "                + str(\"{0:0=3d}\".format(k+1)) + \".mp3\"\n",
    "            file_name = path + str(i+1) + \"-\" + str(j+1) + \"-\" + str(k+1) \\\n",
    "                + \".mp3\"\n",
    "            urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have downloaded all required data but it is downloaded in mp3 format and we need it to change it to wav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just save and run this bash script in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a672ddbd-9aa6-40ce-849e-edece5bee3d8"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "for file in *.mp3; do\n",
    "   ffmpeg -i \"$file\" -acodec pcm_s16le -ac 1 -ar 44100 \"${file%.mp3}\".wav\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our mobile application is recording the audio then it sends it to the server, and the server imports the prediction module and use it to predict the correct verse, and return a string with the order of the Surah and the order of the verse with a dash between them e.g. \"1-2\" means first Surah second verse. We will explain the prediction module after the server module and the training module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install and import flask and werkzeug as our server is flask server and werkzeug to make secure file names that don't cause errors, and import os to save received files and run ffmpeg command to change 3gp, 3gpp, or any other format we want to receive into wav.\n",
    "And of course import our prediction module to use predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, request, redirect, url_for, send_from_directory\n",
    "from werkzeug import secure_filename\n",
    "import prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will write the following code in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = 'uploads'\n",
    "ALLOWED_EXTENSIONS = set(['wav', 'gpp', '3gp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UPLOAD_FOLDER is where we will store the uploaded files and the ALLOWED_EXTENSIONS is the set of allowed file extensions.\n",
    "\n",
    "Why do we limit the extensions that are allowed? You probably don’t want your users to be able to upload everything there if the server is directly sending out the data to the client. That way you can make sure that users are not able to upload HTML files that would cause XSS problems. Also make sure to disallow .php files if the server executes them, but who has PHP installed on their server, right? :)\n",
    "\n",
    "Next the functions that check if an extension is valid and that uploads the file and redirects the user to the URL for the uploaded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allowed_file(filename):\n",
    "  # this has changed from the original example\n",
    "  # because the original did not work for me\n",
    "    return filename[-3:].lower() in ALLOWED_EXTENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About secure_filename function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does that secure_filename() function actually do? Now the problem is that there is that principle called “never trust user input”. This is also true for the filename of an uploaded file. All submitted form data can be forged, and filenames can be dangerous. For the moment just remember: always use that function to secure a filename before storing it directly on the filesystem.\n",
    "\n",
    "what the problem is if we’re not using it? So just imagine someone would send the following information as filename to our application:\n",
    "\n",
    "filename = \"../../../../home/username/.bashrc\"\n",
    "\n",
    "Assuming the number of ../ is correct and we would join this with the UPLOAD_FOLDER the user might have the ability to modify a file on the server’s filesystem he or she should not modify. This does require some knowledge about how the application looks like, but trust me, hackers are patient :)\n",
    "\n",
    "Now let’s look how that function works:\n",
    "\n",
    "$ secure_filename('../../../../home/username/.bashrc')\n",
    "\n",
    "'home_username_.bashrc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want the user to make a POST request to the server to send the file, then make another GET request to get the prediction, and run the server in non-debugging mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        file = request.files['file']\n",
    "        if file and allowed_file(file.filename):\n",
    "            print ('**found file', file.filename)\n",
    "            filename = secure_filename(file.filename)\n",
    "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "            # for browser, add 'redirect' function on top of 'url_for'\n",
    "            return url_for('uploaded_file', filename=filename)\n",
    "    return 'deeply sad story'\n",
    "\n",
    "@app.route('/uploads/<filename>')\n",
    "def uploaded_file(filename):\n",
    "    path = \"uploads/\" + filename\n",
    "    if(path[-4:]==\"3gpp\"):\n",
    "        path2 = path\n",
    "        path = path2[:-4] + \"wav\"\n",
    "        os.system(\"ffmpeg -i \"+path2+\" \"+path)\n",
    "    elif(path[-3:]==\"3gp\"):\n",
    "        path2 = path\n",
    "        path = path2[:-3] + \"wav\"\n",
    "        os.system(\"ffmpeg -i \"+path2+\" \"+path)\n",
    "    print (path)\n",
    "    return prediction.predict(path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False,host= '0.0.0.0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should import requests to make requests to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a file to send it using a post request, Then get the prediction by another get request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '0-1-2.3gpp' # choose what ever file you want to be predicted\n",
    "post_url = \"http://4.4.4.21:5000/\"\n",
    "fin = open(file_name, 'rb')\n",
    "get_url = \"http://4.4.4.21:5000/uploads/\" + file_name\n",
    "#print(get_url)\n",
    "files = {'file': fin}\n",
    "try:\n",
    "  r = requests.post(post_url, files=files)\n",
    "  #print (r.text)\n",
    "  r = requests.get(get_url)\n",
    "  print (r.text)\n",
    "finally:\n",
    "  fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network model for speech recognition and training it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief historical overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech recognition has gotten so much better in the past few decades in the 50s the general consensus amongst computer scientists was that speech signals needed to first be split into little phonetic units then those units could be grouped into words but even though this seemed like it would work well this approach did not give us good results the first-ever speech recognizer was called Audrey by Bell Labs in 1952 it could only recognize spoken numbers between 1 and 9 and was built with analog electronic circuits the renowned scientist and VP at Bell Labs John Pierce banned speech recognition research because the results weren't promising enough but a small group of visionaries at a newly formed team called DARPA went against popular opinion and created a system called harpy it used 15,000 interconnected nodes and each represented all the possible utterances within the domain they used a brute-force search algorithm to map the speech to the right nodes to get the text this approach was slightly better but then big blue invented something called the hidden Markov model HMMs represented utterances as states and probabilistic we predicted what a word was given the fee nomes it was made up of when words like you are pronounced they can have different durations like you or you and HMS captured the plasticity Awards by using a probabilistic approach the hmm pretty much maintained its position as king of speech recognition throughout the 80s and 90s as researchers improved them more and more and some weirdos kept trying this dumb technique called artificial neural networks but of course it didn't get good results one of them this guy Geoffrey Hinton kept on trying out neural networks until all of a sudden a couple years ago it started outperforming everything did people tell you Geoffrey you're wasting your time many times and would you sit back give me another 6 months and I'll prove to you that it works the key was to give it more data and computing power this is deep learning now these deep neural Nets are how services like Siri and echo and Google now hear you speak and with Google's machine learning framework tensorflow.\n",
    "\n",
    "For more informations you can see this video:\n",
    "\n",
    "https://www.youtube.com/watch?v=u9FPqkuoEJ8&t=7s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do Speech Recognition with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big problem is that speech varies in speed. One person might say “hello!” very quickly and another person might say “heeeelllllllllllllooooo!” very slowly, producing a much longer sound file with much more data. Both both sound files should be recognized as exactly the same text — “hello!” Automatically aligning audio files of various lengths to a fixed-length piece of text turns out to be pretty hard.\n",
    "\n",
    "To work around this, we have to use some special tricks and extra precessing in addition to a deep neural network. Let’s see how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Sounds into Bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in speech recognition is obvious — we need to feed sound waves into a computer.\n",
    "\n",
    "But sound is transmitted as waves. How do we turn sound waves into numbers? Let’s use this sound clip.\n",
    "\n",
    "<img src=\"1*6_q1VIVJuavYa-9Uby_L-A.png\">\n",
    "\n",
    "Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave. Let’s zoom in on one tiny part of the sound wave and take a look:\n",
    "\n",
    "<img src=\"1*dqWhWUIzIyOLIqVReTBaiA.png\">\n",
    "\n",
    "To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points:\n",
    "\n",
    "<img src=\"frame_1_delay-1s.gif\">\n",
    "\n",
    "<img src=\"frame_2_delay-1s.gif\">\n",
    "\n",
    "<img src=\"frame_3_delay-3s.gif\">\n",
    "\n",
    "This is called sampling. We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That’s basically all an uncompressed .wav audio file is.\n",
    "\n",
    "“CD Quality” audio is sampled at 44.1khz (44,100 readings per second). But for speech recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech.\n",
    "\n",
    "Lets sample our “Hello” sound wave 16,000 times per second. Here’s the first 100 samples:\n",
    "\n",
    "<img src=\"1*BG4iFbx7qhb5v_JTr958PQ.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Sidebar on Digital Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be thinking that sampling is only creating a rough approximation of the original sound wave because it’s only taking occasional readings. There’s gaps in between our readings so we must be losing data, right?\n",
    "\n",
    "<img src=\"1*KkWfr3a6HtRSZ8-4LUw0kg.png\">\n",
    "\n",
    "But thanks to the Nyquist theorem, we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples — as long as we sample at least twice as fast as the highest frequency we want to record.\n",
    "\n",
    "I mention this only because nearly everyone gets this wrong and assumes that using higher sampling rates always leads to better audio quality. It doesn’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing our Sampled Sound Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an array of numbers with each number representing the sound wave’s amplitude at 1/16,000th of a second intervals.\n",
    "\n",
    "We could feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data.\n",
    "\n",
    "Let’s start by grouping our sampled audio into 20-millisecond-long chunks. Here’s our first 20 milliseconds of audio (i.e., our first 320 samples):\n",
    "\n",
    "<img src=\"1*_qUExEvllTKFhsrITxsa-A.png\">\n",
    "\n",
    "Plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that 20 millisecond period of time:\n",
    "\n",
    "<img src=\"1*ZMxcyjNFqIOVzJRM9BCMWw.png\">\n",
    "\n",
    "This recording is only 1/50th of a second long. But even this short recording is a complex mish-mash of different frequencies of sound. There’s some low sounds, some mid-range sounds, and even some high-pitched sounds sprinkled in. But taken all together, these different frequencies mix together to make up the complex sound of human speech.\n",
    "\n",
    "To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it’s component parts. We’ll break out the low-pitched parts, the next-lowest-pitched-parts, and so on. Then by adding up how much energy is in each of those frequency bands (from low to high), we create a fingerprint of sorts for this audio snippet.\n",
    "\n",
    "Imagine you had a recording of someone playing a C Major chord on a piano. That sound is the combination of three musical notes— C, E and G — all mixed together into one complex sound. We want to break apart that complex sound into the individual notes to discover that they were C, E and G. This is the exact same idea.\n",
    "\n",
    "We do this using a mathematic operation called a Fourier transform. It breaks apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one.\n",
    "\n",
    "The end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch. Each number below represents how much energy was in each 50hz band of our 20 millisecond audio clip:\n",
    "\n",
    "<img src=\"1*2Vg8z3--moE-E7KybJlUPg.png\">\n",
    "\n",
    "But this is a lot easier to see when you draw this as a chart:\n",
    "\n",
    "<img src=\"1*A4CxgdyqYd_nrF3e-7ETWA.png\">\n",
    "\n",
    "If we repeat this process on every 20 millisecond chunk of audio, we end up with a spectrogram (each column from left-to-right is one 20ms chunk):\n",
    "\n",
    "<img src=\"1*bhd7B-s-Qnds3HGV6LOo8A.png\">\n",
    "\n",
    "A spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we’ll actually feed into our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing Characters from Short Sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our audio in a format that’s easy to process, we will feed it into a deep neural network. The input to the neural network will be 20 millisecond audio chunks. For each little audio slice, it will try to figure out the letter that corresponds the sound currently being spoken.\n",
    "\n",
    "<img src=\"1*z1Nf0ES1YVUfdZZGW0PSdQ.png\">\n",
    "\n",
    "We’ll use a recurrent neural network — that is, a neural network that has a memory that influences future predictions. That’s because each letter it predicts should affect the likelihood of the next letter it will predict too. For example, if we have said “HEL” so far, it’s very likely we will say “LO” next to finish out the word “Hello”. It’s much less likely that we will say something unpronounceable next like “XYZ”. So having that memory of previous predictions helps the neural network make more accurate predictions going forward.\n",
    "\n",
    "After we run our entire audio clip through the neural network (one chunk at a time), we’ll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk. Here’s what that mapping looks like for me saying “Hello”:\n",
    "\n",
    "<img src=\"1*d1ktMdOnFOJRKKyjFP6sqQ.png\">\n",
    "\n",
    "Our neural net is predicting that one likely thing I said was “HHHEE_LL_LLLOOO”. But it also thinks that it was possible that I said “HHHUU_LL_LLLOOO” or even “AAAUU_LL_LLLOOO”.\n",
    "\n",
    "We have some steps we follow to clean up this output. First, we’ll replace any repeated characters a single character:\n",
    "\n",
    "    HHHEE_LL_LLLOOO becomes HE_L_LO\n",
    "    HHHUU_LL_LLLOOO becomes HU_L_LO\n",
    "    AAAUU_LL_LLLOOO becomes AU_L_LO\n",
    "\n",
    "Then we’ll remove any blanks:\n",
    "\n",
    "    HE_L_LO becomes HELLO\n",
    "    HU_L_LO becomes HULLO\n",
    "    AU_L_LO becomes AULLO\n",
    "\n",
    "That leaves us with three possible transcriptions — “Hello”, “Hullo” and “Aullo”. If you say them out loud, all of these sound similar to “Hello”. Because it’s predicting one character at a time, the neural network will come up with these very sounded-out transcriptions. For example if you say “He would not go”, it might give one possible transcription as “He wud net go”.\n",
    "\n",
    "The trick is to combine these pronunciation-based predictions with likelihood scores based on large database of written text (books, news articles, etc). You throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic.\n",
    "\n",
    "Of our possible transcriptions “Hello”, “Hullo” and “Aullo”, obviously “Hello” will appear more frequently in a database of text (not to mention in our original audio-based training data) and thus is probably correct. So we’ll pick “Hello” as our final transcription instead of the others. Done!\n",
    "\n",
    "For more general information about machine learning and its applications you can check out the full series of Machine Learning is Fun! from here: https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem called the vanishing gradient problem in RNN. The Gradient exponentially decays as its backpropagated.\n",
    "\n",
    "<img src=\"a.png\">\n",
    "\n",
    "There are two factors that affect the magnitude of gradients - the weights and the activation functions (or more precisely, their derivatives) that the gradient passes through.If either of these factors is smaller than 1, then the gradients may vanish in time; if larger than 1, then exploding might happen.\n",
    "\n",
    "But there exists a solution! Enter the LSTM Cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The LSTM Cell (Long-Short Term Memory Cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've placed no constraints on how our model updates, so its knowledge can change pretty chaotically: at one frame it thinks the characters are in the US, at the next frame it sees the characters eating sushi and thinks they're in Japan, and at the next frame it sees polar bears and thinks they're on Hydra Island.\n",
    "\n",
    "This chaos means information quickly transforms and vanishes, and it's difficult for the model to keep a long-term memory. So what you’d like is for the network to learn how to update its beliefs (scenes without Bob shouldn't change Bob-related information, scenes with Alice should focus on gathering details about her), in a way that its knowledge of the world evolves more gently.\n",
    "\n",
    "It replaces the normal RNN cell and uses an input, forget, and output gate. As well as a cell state \n",
    "\n",
    "<img src=\"lstm2.png\">\n",
    "\n",
    "These gates each have their own set of weight values. The whole thing is differentiable (meaning we compute gradients and update the weights using them) so we can backprop through it\n",
    "\n",
    "We want our model to be able to know what to forget, what to remember. So when new a input comes in, the model first forgets any long-term information it decides it no longer needs. Then it learns which parts of the new input are worth using, and saves them into its long-term memory.\n",
    "\n",
    "And instead of using the full long-term memory all the time, it learns which parts to focus on instead.\n",
    "\n",
    "Basically, we need mechanisms for forgetting, remembering, and attention. That's what the LSTM cell provides us.\n",
    "\n",
    "Which piece of long term memory to remember and forget? We'll use new input and working memory to learn remember gate. Which part of new data should we use and save? Update working memory using attention vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our mission in the project to search for a verse in the Qur'an then determine which verse was it. So instead of generating the text from the audio file then search by this text, we introduced after the LSTM layer a fully connected layer that determine the probability of the input audio to be each verse in the output layer. To be a many to one problem instead of many to many problem.\n",
    "\n",
    "<img src=\"p2iwO.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our NN model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import tflearn and tensorflow to make our model and train it.\n",
    "\n",
    "And import speech_data module that has mfcc_batch_generator which we will talk about later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "import speech_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then tunning our learning parameters to get the best model. We have tried some parameters till we get these accebtable ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0003\n",
    "training_iters = 20  # steps\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the batches to train and validate the model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 20  # mfcc features\n",
    "height = 1500  # (max) length of utterance\n",
    "classes = 21  # digits\n",
    "batch = word_batch =speech_data.mfcc_batch_generator(batch_size=batch_size,\n",
    "                                    data = 0) # data=0 to use training data\n",
    "X, Y = next(batch)\n",
    "trainX, trainY = X, Y\n",
    "testX, testY = X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the network described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network building\n",
    "net = tflearn.input_data([None, width, height])\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam',\n",
    "        learning_rate=learning_rate, loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tflearn.DNN(net, tensorboard_verbose=3)\n",
    "while training_iters>0:\n",
    "    model.fit(trainX, trainY, n_epoch=10, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=batch_size)\n",
    "    _y=model.predict(X)\n",
    "    training_iters -= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to reuse it in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"tflearn.lstm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speech_data util file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import dependancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implement dense_to_one_hot function which util to form the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    return numpy.eye(num_classes)[labels_dense]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implement the mfcc_batch_generator function which is used in getting batches to train the model on them or to validate and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_batch_generator(batch_size=10, data = 0):\n",
    "    path = \"data/quraan/\"\n",
    "    if data == 0: path = \"data/quraan/\"\n",
    "    elif data == 1: path = \"data/quraan_valid/\"\n",
    "    elif data == 2: path = \"data/quraan_test/\"\n",
    "    batch_features = []\n",
    "    labels = []\n",
    "    files = os.listdir(path)\n",
    "    while True:\n",
    "        print(\"loaded batch of %d files\" % len(files))\n",
    "        shuffle(files)\n",
    "        for wav in files:\n",
    "            if not wav.endswith(\".wav\"): continue\n",
    "            wave, sr = librosa.load(path+wav, mono=True)\n",
    "            ### Added code ###\n",
    "            WAV = wav[:-4]\n",
    "            WAV = WAV.split(\"-\")\n",
    "            my_index = 0\n",
    "            if WAV[1] == \"1\": my_index = int(WAV[2]) - 2\n",
    "            elif WAV[1] == \"112\": my_index = 6 + int(WAV[2]) - 1\n",
    "            elif WAV[1] == \"113\": my_index = 10 + int(WAV[2]) - 1\n",
    "            elif WAV[1] == \"112\": my_index = 15 + int(WAV[2]) - 1\n",
    "            ### end ###\n",
    "            label=dense_to_one_hot(my_index,21)\n",
    "            labels.append(label)\n",
    "            mfcc = librosa.feature.mfcc(wave, sr)\n",
    "            # print(np.array(mfcc).shape)\n",
    "            mfcc=np.pad(mfcc,((0,0),(0,1500-len(mfcc[0]))),\n",
    "                        mode='constant', constant_values=0)\n",
    "            batch_features.append(np.array(mfcc))\n",
    "            if len(batch_features) >= batch_size:\n",
    "                # print(np.array(batch_features).shape)\n",
    "                # yield np.array(batch_features), labels\n",
    "                yield batch_features, labels\n",
    "                # basic_rnn_seq2seq inputs must be a sequence\n",
    "                batch_features = []  # Reset for next batch\n",
    "                labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same thing like the training script except we remove the training part, load the saved model, add the predicting part and calculate the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "import speech_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the parameters and determine the size and number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00003\n",
    "batch_size = 256\n",
    "\n",
    "width = 20  # mfcc features\n",
    "height = 1500  # (max) length of utterance\n",
    "classes = 21  # digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the batches to test on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = word_batch =speech_data.mfcc_batch_generator(batch_size=batch_size,\n",
    "                        data=2) # data=2 to use testing data\n",
    "X, Y = next(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulid the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network building\n",
    "net = tflearn.input_data([None, width, height])\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam',\n",
    "            learning_rate=learning_rate, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved weights into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tflearn.DNN(net, tensorboard_verbose=3)\n",
    "model.load(\"tflearn.lstm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the testing inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y=model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the accuracy of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(_y)\n",
    "n_true = 0\n",
    "for i in range(n):\n",
    "    if(np.argmax(_y[i])==np.argmax(Y[i])):\n",
    "        n_true += 1\n",
    "print(\"Testing accuracy = \" + str(1.0*n_true/n)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The prediction module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will talk about the prediction module used in the server.\n",
    "\n",
    "It is almost like the testing module except it doesn't compare the output with a label nor calculating the accuracy. It just returns the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed modules\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tflearn\n",
    "\n",
    "def mfcc_batch_generator(path):\n",
    "    batch_features = []\n",
    "    wave, sr = librosa.load(path, mono=True)\n",
    "    mfcc = librosa.feature.mfcc(wave, sr)\n",
    "    mfcc = np.pad(mfcc,((0,0),(0,1500-len(mfcc[0]))), mode='constant',\n",
    "                  constant_values=0)\n",
    "    batch_features.append(np.array(mfcc))\n",
    "    return batch_features\n",
    "\n",
    "# Setting the parameters\n",
    "learning_rate = 0.00003\n",
    "width = 20\n",
    "height = 1500\n",
    "classes = 21\n",
    "\n",
    "#building the network\n",
    "net = tflearn.input_data([None, width, height])\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, classes, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=learning_rate,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "#load the model\n",
    "model = tflearn.DNN(net, tensorboard_verbose=3)\n",
    "model.load(\"tflearn.lstm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will predict using the mfcc_batch_generator and model.predict functions to predict the output, Then we format the output in the standard way we mentioned in above -the order of the Surah then dash then the order of the verse in the Surah.\n",
    "\n",
    "Here we are predicting verses of Al-Fatiha, Al-Ikhlas, Al-Falaq, and An-Nas Surahs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path):\n",
    "    X = mfcc_batch_generator(path)\n",
    "    _y=model.predict(X)\n",
    "    y = np.argmax(_y[0])\n",
    "    if (y<6):\n",
    "        return \"1-\"+str(y+2)\n",
    "    elif (y<10):\n",
    "        y -= 6\n",
    "        return \"112-\"+str(y+1)\n",
    "    elif (y<15):\n",
    "        y -= 10\n",
    "        return \"113-\"+str(y+1)\n",
    "    else:\n",
    "        y -= 15\n",
    "        return \"114-\"+str(y+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
